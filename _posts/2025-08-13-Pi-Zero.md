---
layout: post
title: Pi-Zero Î¶¨Î∑∞
tags: Transformer, DL, GenerativeModel, VLA
published: true
math: true
date: 2025-08-13 09:00 +0900
---


[arxiv.org](https://arxiv.org/pdf/2410.24164)

# œÄ0: A Vision-Language-Action Flow Model for General Robot Control

# Introduction

### Challenges in generalist robot policy

- Large scale
- Model architecture
- Training recipe ‚Üí pre-training/post-training

### Action head in VLA models

- Autoregressive discretization
    - RT-2
    - OpenVLA
    - TinyVLA
    - Inefficient and slow inference speed (3‚Äì5 Hz)
    - Quantization disrupts action continuity
- Diffusion
    - Octo(is not VLA), OneDP, TinyVLA, DiVLA
    - Produce complex actions
    - Inherently slow inference, but can be improved up to 50~62Hz
    - Flow matching

### Flow matching

- Continuous Flow
- Discrete Flow
- Flow matching

# Pi-zero model

### Pretrained VLM

PaliGemma[[https://arxiv.org/pdf/2407.07726](https://arxiv.org/pdf/2407.07726)]

### Flow matching

- variant of diffusion (denoising score matching)
- **diffusion-style (flow matching) loss applied on individual sequence elements** instead of standard cross-entropy loss
for decoder-only transformer
    - [[Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Mode](https://arxiv.org/pdf/2408.11039)l]
    - single transformer using multiple objectives, with tokens corresponding to **continuous outputs supervised via a flow matching loss** and tokens corresponding to **discrete outputs supervised via a cross-entropy loss**
- Use **separate set of weights for tokens** corresponding to diffusion
    - [[Playground v3: Improving Text-to-Image Alignment with Deep-Fusion Large
    Language Models](https://arxiv.org/pdf/2409.10695)]
    - MoE (1) image/text inputs **(2) robotics-specific inputs/outputs (aka action expert)**


### Problem Definition
- Input
    - Observation ùê®t = [ùêàt 1,‚Ä¶,ùêàtn,‚Ñìt,ùê™t]
        - RGB Image tokens (2~3 images per robot)
        - Sequence of language tokens (language command)
        - Vector of joint angles (robot‚Äôs proprioceptive state)
- Output
    - Action chunk ùêÄt = [ùêöt,ùêöt+1,‚Ä¶,ùêöt+H‚àí1]
        - H=50
        - Action token

### Blockwise Causal Attention
![alt text](https://github-production-user-asset-6210df.s3.amazonaws.com/100525192/481903855-4837f382-3fbe-49c6-83a6-bc7104a523fc.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20250826%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250826T040918Z&X-Amz-Expires=300&X-Amz-Signature=837e3f8dcc534443f4f229ac88563daf432c8692f4888c6dcc0716b82de36da2&X-Amz-SignedHeaders=host)

### Training / Inference
- Training: CNF loss
![alt text](https://github-production-user-asset-6210df.s3.amazonaws.com/100525192/481904467-c072b9ab-b96c-4b10-9477-f061897cb779.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20250826%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250826T041159Z&X-Amz-Expires=300&X-Amz-Signature=3293ac24b51b328f63446ae71da61e556de7513d2a511a9bce2d60ab7ad7dd7a&X-Amz-SignedHeaders=host)
- Inference: 
![alt text](https://github-production-user-asset-6210df.s3.amazonaws.com/100525192/481904721-eeab463f-4ca7-4f62-a838-cb155ac72d8f.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20250826%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250826T041301Z&X-Amz-Expires=300&X-Amz-Signature=1ea02e7b84cfd45a72be86859b0da2cb8104e8dab19d9cdbb5b3fb42fa054aa0&X-Amz-SignedHeaders=host)

#### Remaining Questions
- How pre-training dataset should be composed?
- What type of data is more helpful to add?
- How it should be weighted?
- Universality in distinct domains (data/robots)?
#### Limitations
- Generate H-step action chunk at once
- Cannot modify until action ends
- Still slow speed for action executing